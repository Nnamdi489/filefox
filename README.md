
# ğŸ¦Š FileFox (web chatbot with document analysis and cms integration)

**FileFox** is a full stack AI chatbot that answers questions based on uploaded documents/files (**PDF, DOCX, CSV**) and CMS content using a vector db and local llm (phi3).

---

## Features

- Upload & chat with your documents  
- Local AI via **Ollama (phi3:latest)**  
- Smart search with **Qdrant vector DB**  
- Cloud file storage on **DigitalOcean Spaces**  
- Modern and responsive design (works on desktop and mobile).


---

## Tech Stack

**Backend:** FastAPI Â· Qdrant Â· Ollama Â· sentence-transformers Â· DigitalOcean Spaces  
**Frontend:** React + Vite (deployed on vercel)

---

## Prerequisites

Before you start, install:

- **Python 3.10+**
- **Node.js 18+**
- **Ollama** â†’ [Download here](https://ollama.com/download)

After installing Ollama, pull the model:
```bash
ollama pull phi3:latest (ollama model recommended for computer with less storage runs smoothly on macbook M1,M2 and windows)

Youâ€™ll also need:
	â€¢	Qdrant Cloud (Free)
	â€¢	DigitalOcean Spaces ($5/month) (optional for file storage , you can decide to entirely run it with your local machine storage)


ğŸ“ Project Structure

filefox/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # Main FastAPI app
â”‚   â”œâ”€â”€ document_parser.py     # Parse PDF/DOCX/CSV
â”‚   â”œâ”€â”€ embeddings.py          # Generate embeddings
â”‚   â”œâ”€â”€ qdrant_utils.py        # Vector database operations
â”‚   â”œâ”€â”€ s3_utils.py            # DigitalOcean Spaces upload
â”‚   â”œâ”€â”€ llm_client.py          # Ollama integration
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â””â”€â”€ .env                   # Your secret keys (don't commit , it should be on your .gitignore)
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.jsx            # Main React component
    â”‚   â”œâ”€â”€ App.css            # Styling
    â”‚   â”œâ”€â”€ main.jsx           # React entry point
    â”‚   â””â”€â”€ index.css          # styles
    â”œâ”€â”€ package.json
    â””â”€â”€ .env                   # Frontend config (vite_api-url, it can either by default localhost or ngrok please don't commit)
	

	  How It Works

Upload: User uploads a document (PDF, DOCX, or CSV)
Parse: Backend extracts text and splits into chunks
Embed: Each chunk is converted to a vector (embedding)
Store: Vectors are stored in Qdrant, files in DO Spaces
Query: User asks a question
Search: Question is embedded and similar chunks are found
Generate: Ollama generates an answer using retrieved chunks
Display: Answer are shown to user


    Tips & Best Practices
For Better Answers:
Upload documents with clear, well-structured text
Ask specific questions rather than vague ones
If answer is poor, try rephrasing your question

File Guidelines:
PDFs: Works best with text-based PDFs (not scanned images)
DOCX: Plain text documents work best
CSV: Good for structured data, FAQs, etc.

Performance:
First query after upload may be slower (model loading)
Larger files take longer to process
Llama phi3:lastest is fast but limited - for better quality, use larger models (preferable on a server with atleast 1Tb storage)

Cost Management:
Qdrant free tier: 1GB storage
DO Spaces: $5/month for 250GB
Ollama: free (runs locally)
Total monthly cost: ~$5

**Need Help?** Refer to SETUP_CHECKLIST.md for detailed checklist guide!